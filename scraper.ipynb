{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81f0ce51",
   "metadata": {},
   "source": [
    "# Finance News\n",
    "from yahoo finance: https://finance.yahoo.com/topic/latest-news/\n",
    "\n",
    "\n",
    "based on these works\n",
    "- https://medium.com/@georgemichaeldagogomaynard/web-scraping-dynamic-sites-with-both-selenium-and-beautiful-soup-dim-c679743018de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aa1fd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "# Selenium modules for controlling Chrome browser\n",
    "from selenium.webdriver import Chrome  # For initializing and controlling the Chrome browser\n",
    "from selenium import webdriver  # Provides access to the webdriver, allowing interaction with web browsers\n",
    "from selenium.webdriver.chrome.options import Options  # For configuring Chrome browser options (e.g., headless mode)\n",
    "from selenium.webdriver.chrome.service import Service  # For managing the ChromeDriver service (e.g., starting, stopping)\n",
    "\n",
    "# Selenium modules for interacting with web elements\n",
    "from selenium.webdriver.common.by import By  # For locating elements on a webpage (e.g., By.ID, By.XPATH)\n",
    "from selenium.webdriver.support.ui import Select  # For interacting with <select> HTML elements (dropdowns)\n",
    "from selenium.webdriver.support.ui import WebDriverWait  # For implementing explicit waits until a condition is met\n",
    "from selenium.webdriver.support import expected_conditions as EC  # For defining conditions to wait for (e.g., element visibility)\n",
    "\n",
    "# Other useful libraries\n",
    "from fake_useragent import UserAgent  # For generating random user agents to mimic different browsers\n",
    "import time  # For adding delays (e.g., time.sleep) during the script execution\n",
    "import requests  # For making HTTP requests to interact with websites directly without using a browser\n",
    "from bs4 import BeautifulSoup  # For parsing and extracting data from HTML content\n",
    "import pandas as pd  # For data manipulation, analysis, and creating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498eed9e",
   "metadata": {},
   "source": [
    "## Experimenting with Scraper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641c3bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrolled 1/3 times\n",
      "Scrolled 2/3 times\n",
      "Scrolled 3/3 times\n",
      "Found 70 news articles\n",
      "Palantir uses the '5 Whys' approach to problem solving — here's how it works\n",
      "Business Insider \n",
      "https://finance.yahoo.com/news/palantir-uses-5-whys-approach-110103609.html\n",
      "Brent D. Griffiths\n",
      "Sat, November 29, 2025 at 6:01 PM GMT+7\n",
      "[{'symbol': 'PLTR', 'change': '+1.62%'}]\n",
      "\n",
      "\n",
      "HELOC rates today, November 29, 2025: Rates fall as holiday cash needs rise\n",
      "Yahoo Personal Finance \n",
      "https://finance.yahoo.com/personal-finance/mortgages/article/heloc-rates-today-saturday-november-29-2025-110050848.html\n",
      "Hal Bundrick, CFP®  · Senior Writer   Laura Grace Tarpley  · Lead Editor and Content Strategist, Mortgages\n",
      "Sat, November 29, 2025 at 6:00 PM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "Mortgage and refinance interest rates today, November 29, 2025: Could the next move be below 6%?\n",
      "Yahoo Personal Finance \n",
      "https://finance.yahoo.com/personal-finance/mortgages/article/mortgage-refinance-interest-rates-today-saturday-november-29-2025-110028470.html\n",
      "Hal Bundrick, CFP®  · Senior Writer   Laura Grace Tarpley  · Lead Editor and Content Strategist, Mortgages\n",
      "Sat, November 29, 2025 at 6:00 PM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "Best high-yield savings interest rates today, November 29, 2025 (Earn up to 4.3% APY)\n",
      "Yahoo Personal Finance \n",
      "https://finance.yahoo.com/personal-finance/article/best-high-yield-savings-interest-rates-today-saturday-november-29-2025-110017107.html\n",
      "Casey Bond  · Lead Editor and Content Strategist, Banking\n",
      "Sat, November 29, 2025 at 6:00 PM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "ChatGPT was unveiled 3 years ago, kicking off the AI revolution. For investors, it did even more.\n",
      "Yahoo Finance \n",
      "https://finance.yahoo.com/news/chatgpt-was-unveiled-3-years-ago-kicking-off-the-ai-revolution-for-investors-it-did-even-more-110014912.html\n",
      "Myles Udland    · Head of News\n",
      "Sat, November 29, 2025 at 6:00 PM GMT+7\n",
      "[{'symbol': 'PLTR', 'change': '+1.62%'}, {'symbol': 'NVDA', 'change': '-1.81%'}, {'symbol': 'META', 'change': '+2.26%'}]\n",
      "\n",
      "\n",
      "Best money market account rates today, November 29, 2025 (best account provides 4.26% APY)\n",
      "Yahoo Personal Finance \n",
      "https://finance.yahoo.com/personal-finance/banking/article/best-money-market-account-rates-today-saturday-november-29-2025-110014405.html\n",
      "Casey Bond  · Lead Editor and Content Strategist, Banking\n",
      "Sat, November 29, 2025 at 6:00 PM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "Best CD rates today, November 29, 2025 (best account provides 4.1% APY)\n",
      "Yahoo Personal Finance \n",
      "https://finance.yahoo.com/personal-finance/banking/article/best-cd-rates-today-saturday-november-29-2025-110002339.html\n",
      "Casey Bond  · Lead Editor and Content Strategist, Banking\n",
      "Sat, November 29, 2025 at 6:00 PM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "The AI bubble debate is slowly making its way to Washington\n",
      "Business Insider \n",
      "https://finance.yahoo.com/news/ai-bubble-debate-slowly-making-102206636.html\n",
      "Bryan Metzger\n",
      "Sat, November 29, 2025 at 5:22 PM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "What to know ahead of Trump’s 'high stakes' Fed chair nomination\n",
      "USA TODAY \n",
      "https://finance.yahoo.com/news/know-ahead-trump-high-stakes-100359410.html\n",
      "Rachel Barber, USA TODAY\n",
      "Sat, November 29, 2025 at 5:03 PM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "ACA premiums to surge in 2026. Here’s what to do about it\n",
      "Quartz \n",
      "https://finance.yahoo.com/news/aca-premiums-surge-2026-100000408.html\n",
      "Deborah Kearns\n",
      "Sat, November 29, 2025 at 5:00 PM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "The US auto industry is bracing for an EV winter. Here's why.\n",
      "Business Insider \n",
      "https://finance.yahoo.com/news/us-auto-industry-bracing-ev-090502821.html\n",
      "Tom Carter\n",
      "Sat, November 29, 2025 at 4:05 PM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "Elon Musk has started work toward his $1 trillion Tesla pay package. But 2 loopholes foreshadow how it could be a bust for shareholders\n",
      "Fortune \n",
      "https://finance.yahoo.com/news/elon-musk-started-toward-1-090000571.html\n",
      "Shawn Tully\n",
      "Sat, November 29, 2025 at 4:00 PM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "EV tax to drive up price of new petrol cars\n",
      "The Telegraph \n",
      "https://finance.yahoo.com/news/ev-tax-drive-price-petrol-090000082.html\n",
      "Szu Ping Chan\n",
      "Sat, November 29, 2025 at 4:00 PM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "What the CEO of the world’s largest data center company—with 273 locations in 36 countries—predicts will drive the business forward\n",
      "Fortune \n",
      "https://finance.yahoo.com/news/ceo-world-largest-data-center-090000709.html\n",
      "Diane Brady, Sharon Goldman\n",
      "Sat, November 29, 2025 at 4:00 PM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "National debt crisis will be averted by governments ‘mobilizing and encouraging’ private wealth to fill budget holes, says UBS\n",
      "Fortune \n",
      "https://finance.yahoo.com/news/national-debt-crisis-averted-governments-081200632.html\n",
      "Eleanor Pringle\n",
      "Sat, November 29, 2025 at 3:12 PM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "Wall Street is relying on the Supreme Court to protect the Fed. Is that wishful thinking?\n",
      "CNN Business \n",
      "https://finance.yahoo.com/news/wall-street-relying-supreme-court-080058641.html\n",
      "Analysis by Bryan Mena, CNN\n",
      "Sat, November 29, 2025 at 3:00 PM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "Micron to invest $9.6 billion in Japan to build AI memory chip plant, Nikkei reports\n",
      "Reuters \n",
      "https://finance.yahoo.com/news/micron-invest-9-6-billion-075214757.html\n",
      "Reuters\n",
      "Sat, November 29, 2025 at 2:52 PM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "‘Does Reeves have a plan for growth?’ Your Budget questions answered\n",
      "The Telegraph \n",
      "https://finance.yahoo.com/news/does-reeves-plan-growth-budget-070000118.html\n",
      "Rachel Obordo\n",
      "Sat, November 29, 2025 at 2:00 PM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "Has XRP Finally Bottomed? Key Support Holds as Wave-5 Breakout Trigger Nears\n",
      "CoinDesk \n",
      "https://finance.yahoo.com/news/xrp-finally-bottomed-key-support-064611434.html\n",
      "Shaurya Malwa\n",
      "Sat, November 29, 2025 at 1:46 PM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "VC Kara Nortman bet early on women’s sports, and now she’s creating the market\n",
      "TechCrunch \n",
      "https://finance.yahoo.com/news/vc-kara-nortman-bet-early-061751354.html\n",
      "Connie Loizos\n",
      "Sat, November 29, 2025 at 1:17 PM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "Bitcoin's 'Coinbase Premium' Flips Positive After Weeks in the Red\n",
      "CoinDesk \n",
      "https://finance.yahoo.com/news/bitcoins-coinbase-premium-flips-positive-061528214.html\n",
      "Shaurya Malwa\n",
      "Sat, November 29, 2025 at 1:15 PM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "Could the EU’s frozen-assets plan really destabilise European bond markets?\n",
      "Euronews \n",
      "https://finance.yahoo.com/news/could-eu-frozen-assets-plan-060046046.html\n",
      "Doloresz Katanich\n",
      "Sat, November 29, 2025 at 1:00 PM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "Ryanair scraps VIP scheme after customers take too many cheap flights\n",
      "The Telegraph \n",
      "https://finance.yahoo.com/news/ryanair-scraps-vip-scheme-customers-060000342.html\n",
      "Christopher Jasper\n",
      "Sat, November 29, 2025 at 1:00 PM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "AP Top Extended Financial Headlines at 5:42 a.m. EST\n",
      "Associated Press Finance \n",
      "https://finance.yahoo.com/news/ap-top-extended-financial-headlines-050000981.html\n",
      "Associated Press Finance\n",
      "Sat, November 29, 2025 at 12:00 PM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "AP Top Financial News at 5:42 a.m. EST\n",
      "Associated Press Finance \n",
      "https://finance.yahoo.com/news/ap-top-financial-news-5-050000712.html\n",
      "Associated Press Finance\n",
      "Sat, November 29, 2025 at 12:00 PM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "This African nation built its development on diamonds. Now it's crashing down\n",
      "Associated Press Finance \n",
      "https://finance.yahoo.com/news/african-nation-built-development-diamonds-040543518.html\n",
      "SELLO MOTSETA and FARAI MUTSAKA\n",
      "Sat, November 29, 2025 at 11:05 AM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "A couple earning six figures from 28 rental units explains how they use a 'buy box' to ensure positive cash flow\n",
      "Business Insider \n",
      "https://finance.yahoo.com/news/couple-earning-six-figures-28-173002954.html\n",
      "Kathleen Elkins\n",
      "Sat, November 29, 2025 at 10:41 AM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "Consumers should do their research before giving in to Travel Tuesday temptation\n",
      "Associated Press Finance \n",
      "https://finance.yahoo.com/news/consumers-research-giving-travel-tuesday-025918209.html\n",
      "CORA LEWIS\n",
      "Sat, November 29, 2025 at 9:59 AM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "Michael Burry just started a massive group chat on Substack, and it's as chaotic as you'd expect\n",
      "Business Insider \n",
      "https://finance.yahoo.com/news/michael-burry-just-started-massive-013920202.html\n",
      "Theron Mohamed\n",
      "Sat, November 29, 2025 at 8:39 AM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "Shoppers are on pace to break Black Friday online spending records and use AI more than ever as sales hit $8.6 billion\n",
      "Business Insider \n",
      "https://finance.yahoo.com/news/shoppers-pace-break-black-friday-003952801.html\n",
      "Pranav Dixit\n",
      "Sat, November 29, 2025 at 7:39 AM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "3 Highest-Yielding Dividend Kings To Buy, Hold, and Forget\n",
      "Barchart \n",
      "https://finance.yahoo.com/news/3-highest-yielding-dividend-kings-000002656.html\n",
      "Rick Orford\n",
      "Sat, November 29, 2025 at 7:00 AM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "Tesla loses some AI staff to a new robotics startup\n",
      "Business Insider \n",
      "https://finance.yahoo.com/news/tesla-loses-ai-staff-robotics-235141020.html\n",
      "Grace Kay\n",
      "Sat, November 29, 2025 at 6:51 AM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "Bridgewater's Greg Jensen echoes Michael Burry on Nvidia's AI chips — and says they could help make themselves obsolete\n",
      "Business Insider \n",
      "https://finance.yahoo.com/news/bridgewaters-greg-jensen-echoes-michael-233405826.html\n",
      "Theron Mohamed\n",
      "Sat, November 29, 2025 at 6:34 AM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "‘Working for tariffs’: A small Christmas supply firm in Utah is struggling with big bills\n",
      "NBC News \n",
      "https://finance.yahoo.com/news/working-tariffs-small-christmas-supply-233109769.html\n",
      "Maya Huter\n",
      "Sat, November 29, 2025 at 6:31 AM GMT+7\n",
      "[]\n",
      "\n",
      "\n",
      "BC-Nikkei 225 Futures\n",
      "Associated Press Finance \n",
      "https://finance.yahoo.com/news/bc-nikkei-225-futures-233015433.html\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 83\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;124;03mnews_detail_page = requests.get(news_detail_url, headers=headers)\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03mnews_detail_soup = BeautifulSoup(news_detail_page.content, 'html.parser')\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Migrate to selenium\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(news_detail_url)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     85\u001b[0m     WebDriverWait(driver, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39muntil(EC\u001b[38;5;241m.\u001b[39mpresence_of_element_located((By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msection[class*=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mticker-list yf-1sa9i6j\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "File \u001b[1;32mc:\\Users\\tanap\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:483\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    466\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Navigate the browser to the specified URL in the current window or\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03m    tab.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;124;03m    >>> driver.get(\"https://example.com\")\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mGET, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: url})\n",
      "File \u001b[1;32mc:\\Users\\tanap\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:455\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    452\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m    453\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[1;32m--> 455\u001b[0m response \u001b[38;5;241m=\u001b[39m cast(RemoteConnection, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor)\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[1;32mc:\\Users\\tanap\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:407\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    405\u001b[0m trimmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trim_large_entries(params)\n\u001b[0;32m    406\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, command_info[\u001b[38;5;241m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[1;32m--> 407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(command_info[\u001b[38;5;241m0\u001b[39m], url, body\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[1;32mc:\\Users\\tanap\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:431\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    428\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_config\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[1;32m--> 431\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn\u001b[38;5;241m.\u001b[39mrequest(method, url, body\u001b[38;5;241m=\u001b[39mbody, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_config\u001b[38;5;241m.\u001b[39mtimeout)\n\u001b[0;32m    432\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\tanap\\anaconda3\\Lib\\site-packages\\urllib3\\_request_methods.py:143\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[1;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[0;32m    136\u001b[0m         method,\n\u001b[0;32m    137\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw,\n\u001b[0;32m    141\u001b[0m     )\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_body(\n\u001b[0;32m    144\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[0;32m    145\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\tanap\\anaconda3\\Lib\\site-packages\\urllib3\\_request_methods.py:278\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m, content_type)\n\u001b[0;32m    276\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[1;32m--> 278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw)\n",
      "File \u001b[1;32mc:\\Users\\tanap\\anaconda3\\Lib\\site-packages\\urllib3\\poolmanager.py:459\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    457\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, u\u001b[38;5;241m.\u001b[39mrequest_uri, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    461\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[1;32mc:\\Users\\tanap\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tanap\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\tanap\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:565\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    562\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 565\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    568\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Users\\tanap\\anaconda3\\Lib\\http\\client.py:1430\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1428\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1430\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1431\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1432\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\tanap\\anaconda3\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tanap\\anaconda3\\Lib\\http\\client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\tanap\\anaconda3\\Lib\\socket.py:719\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot read from timed out object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    720\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Testing the scraper (replace print with storing data in a structured format later)\n",
    "# TODO: make this shit works (scrape ticker from inside article)\n",
    "import re\n",
    "\n",
    "news_uri = \"https://finance.yahoo.com/topic/latest-news/\"\n",
    "\n",
    "# Use selenium for \"dynamic\" content loading \n",
    "options=Options()\n",
    "options.add_argument(\"--headless\")  # Run Chrome in headless mode (without GUI)\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "# Block images to speed up loading\n",
    "prefs = {\"profile.managed_default_content_settings.images\": 2}\n",
    "options.add_experimental_option(\"prefs\", prefs)\n",
    "options.page_load_strategy = 'eager'  \n",
    "\n",
    "# Add headers or some bs to mimic a browser request to avoid 429 (Too Many Requests)\n",
    "ua = UserAgent()\n",
    "userAgent = ua.random\n",
    "options.add_argument(f'user-agent={userAgent}')\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "try: \n",
    "    driver.get(news_uri)\n",
    "    time.sleep(5)  # Wait for the page to load completely\n",
    "    \n",
    "    # Scroll to the bottom to load more news\n",
    "    scroll_count = 3\n",
    "    for i in range(scroll_count):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)  # Wait for new content to load after scrolling\n",
    "        print (f\"Scrolled {i+1}/{scroll_count} times\")\n",
    "    page = driver.page_source\n",
    "    news_soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    #print (page.status_code)  # should be 200 if successful\n",
    "    #print (page.content[:100])  \n",
    "\n",
    "\n",
    "    news_list = news_soup.find('ul', class_= \"stream-items yf-9xydx9\")\n",
    "    news_listings = news_list.find_all('li', class_= \"stream-item story-item yf-9xydx9\")\n",
    "    print (f\"Found {len(news_listings)} news articles\")\n",
    "\n",
    "    # TODO: catch None conditions, fix duplicates issue, add ticker percentage change\n",
    "    for news in news_listings:\n",
    "        # Want to scrape: title, link, publisher, tickers \n",
    "        print(news.find('h3').text)  # Article's title\n",
    "        \n",
    "        # Use regex to match 'publishing' followed by any other class names\n",
    "        print(news.find('div', class_=re.compile(r'publishing.*')).text.split('•')[0]) # Publisher name\n",
    "        \n",
    "        '''\n",
    "        # Extract tickers and their percentage changes\n",
    "        if news.find('div', class_='taxonomy-links'):  \n",
    "            tickers = []\n",
    "            for ticker in news.find('div', class_='taxonomy-links').find_all('span', class_=re.compile(r'ticker-wrapper.*')):\n",
    "                # get data from the fin-streamer element\n",
    "                streamer = ticker.find('fin-streamer', {'data-field': 'regularMarketChangePercent'})\n",
    "                if streamer:\n",
    "                    print (streamer)\n",
    "                    symbol = streamer.get('data-symbol')\n",
    "                    change = streamer.get_text(strip=True) # TODO: fix this shit only loading for the first news (maybe increase wait time or scroll more)\n",
    "                    tickers.append({'symbol': symbol, 'change': change})\n",
    "                else:\n",
    "                    # Fallback if streamer is not found\n",
    "                    tickers.append({'symbol': ticker.text.strip(), 'change': None})\n",
    "                \n",
    "            #print (', '.join(tickers))  # All tickers in one line\n",
    "            print (tickers)\n",
    "        else:\n",
    "            print(\"No tickers found\")\n",
    "        '''\n",
    "        print (news.find('a')['href']) # link to the article\n",
    "        \n",
    "        #bug is after here?\n",
    "        #TODO: make this process faster\n",
    "        news_detail_url = news.find('a')['href']\n",
    "        \n",
    "        '''\n",
    "        news_detail_page = requests.get(news_detail_url, headers=headers)\n",
    "        news_detail_soup = BeautifulSoup(news_detail_page.content, 'html.parser')\n",
    "        '''\n",
    "        # Migrate to selenium\n",
    "        driver.get(news_detail_url)\n",
    "        try:\n",
    "            WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"section[class*='ticker-list yf-1sa9i6j']\")))\n",
    "        except:\n",
    "            pass \n",
    "            \n",
    "        news_detail_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # Start looking inside each article!\n",
    "        # writers, time published, article content, \n",
    "        # TODO: might use ticker-wrapper from <div class= \"slide\"> instead\n",
    "        print(news_detail_soup.find('div', class_=re.compile(r'byline-attr-author.*')).text.strip()) # Authors\n",
    "        print(news_detail_soup.find('time').text) # Time published\n",
    "        \n",
    "        # bug is caused before here?\n",
    "        \n",
    "        # Stock tickers from div stock-story-tickers and remaining-stocks-container (or section ticker-list)\n",
    "        #print(news_detail_soup.find('section', class_=re.compile(r'ticker-list.*')  ))\n",
    "        stock_tickers_div = news_detail_soup.find('div', class_=re.compile(r'stock-story-tickers.*'))\n",
    "        #print (stock_tickers_div)\n",
    "        tickers = []\n",
    "        if stock_tickers_div:\n",
    "            for ticker in stock_tickers_div.find_all('div', class_='slide'):\n",
    "                streamer = ticker.find('fin-streamer', {'data-field': 'regularMarketChangePercent'})\n",
    "                if streamer:\n",
    "                    symbol = streamer.get('data-symbol')    \n",
    "                    change = streamer.get_text(strip=True) \n",
    "                    tickers.append({'symbol': symbol, 'change': change})\n",
    "        \n",
    "        print (tickers)\n",
    "        remaining_stocks_div = news_detail_soup.find('div', class_=re.compile(r'remaining-stocks-container.*'))\n",
    "        \n",
    "        '''      \n",
    "        # Body: for each paragraph in the article content\n",
    "        body_wrapper = news_detail_soup.find('div', class_='bodyItems-wrapper')\n",
    "        if body_wrapper:\n",
    "            for para in body_wrapper.find_all('p'):\n",
    "                # Check if the paragraph is inside 'read-more-wrapper' to avoid duplication\n",
    "                if para.find_parent('div', class_='read-more-wrapper'): # if parent is read-more-wrapper\n",
    "                    continue\n",
    "                print(para.text.strip())\n",
    "                \n",
    "        # 'Read more' section\n",
    "        read_more_wrapper = news_detail_soup.find('div', class_='read-more-wrapper')\n",
    "        if read_more_wrapper:\n",
    "            for para in read_more_wrapper.find_all('p'): \n",
    "                print(para.text.strip())\n",
    "        '''\n",
    "        \n",
    "        print (\"\\n\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "    \n",
    "# ul and li(class = story-item) tags are used for listing news articles\n",
    "# For the content of each news article: <div class=content> ->...(below)..\n",
    "# inside div(class = footer) ->  <div(class = publishing)> for publisher name and time (1st & 2nd text)\n",
    "# inside div(class = footer) -> <div(class = taxonomy-links)> -> <div(class=ticker-wrapper)> for each ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ec73fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html class=\"smartphone neo-green dock-upscale\" data-color-scheme=\"auto\" data-color-theme-enabled=\"true\" lang=\"en-US\" theme=\"auto\">\\n <head>\\n  <meta charset=\"utf-8\"/>\\n  <meta content=\"g'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.prettify()[:200] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "297c808a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n",
      "Connected to database: finance_news_db, collection: numerous_articles\n"
     ]
    }
   ],
   "source": [
    "# Connect to MongoDB\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "db_username = os.getenv(\"db_username\")\n",
    "db_password = os.getenv(\"db_password\")\n",
    "db_host = os.getenv(\"db_host\")\n",
    "\n",
    "if not all([db_username, db_password, db_host]):\n",
    "    print(\"Please set db_username, db_password, and db_host in your .env file\")\n",
    "else:\n",
    "    uri = f\"mongodb+srv://{db_username}:{db_password}@{db_host}\"\n",
    "    client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "\n",
    "    try:\n",
    "        client.admin.command('ping')\n",
    "        print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "        \n",
    "        # Create/Select database and collection\n",
    "        db = client[\"finance_news_db\"] \n",
    "        collection = db[\"numerous_articles\"]\n",
    "        print(f\"Connected to database: {db.name}, collection: {collection.name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8f1e7c",
   "metadata": {},
   "source": [
    "# Real Scraper w/connection to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e08ebb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news list via Selenium...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching news list via Selenium...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(news_uri)\n\u001b[1;32m---> 20\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m) \u001b[38;5;66;03m# Wait for page to load\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Scroll to load more news\u001b[39;00m\n\u001b[0;32m     23\u001b[0m scroll_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "# Working scraper!\n",
    "import re\n",
    "\n",
    "news_uri = \"https://finance.yahoo.com/topic/latest-news/\"\n",
    "\n",
    "# We use Selenium because the 'fin-streamer' percentages are loaded via JavaScript\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\") \n",
    "ua = UserAgent()\n",
    "userAgent = ua.random\n",
    "options.add_argument(f'user-agent={userAgent}')\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Headers for the requests library\n",
    "headers = {'User-Agent': userAgent}\n",
    "\n",
    "try:\n",
    "    print(\"Fetching news list via Selenium...\")\n",
    "    driver.get(news_uri)\n",
    "    time.sleep(3) # Wait for page to load\n",
    "    \n",
    "    # Scroll to load more news\n",
    "    scroll_count = 30\n",
    "    for i in range(scroll_count):\n",
    "        # Scroll incrementally to ensure elements trigger their data load\n",
    "        driver.execute_script(\"window.scrollBy(0, 1000);\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2) # Wait for new items to load\n",
    "        print(f\"Scrolled {i+1}/{scroll_count}\")\n",
    "        \n",
    "    page_source = driver.page_source\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "news_soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "news_list = news_soup.find('ul', class_= \"stream-items yf-9xydx9\")\n",
    "news_listings = news_list.find_all('li', class_= \"stream-item story-item yf-9xydx9\")\n",
    "print(f\"Found {len(news_listings)} articles.\")\n",
    "\n",
    "for news in news_listings:\n",
    "    news_data = {}\n",
    "    # From the main listing page: want to scrape title, link, publisher, tickers(???)\n",
    "    # Title\n",
    "    title = news.find('h3')\n",
    "    if not title: continue # Skip if malformed\n",
    "    news_data['title'] = title.text\n",
    "    \n",
    "    # Publisher\n",
    "    publisher = news.find('div', class_=re.compile(r'publishing.*'))\n",
    "    news_data['publisher'] = publisher.text.split('•')[0] if publisher else \"Unknown\"\n",
    "    \n",
    "    # Tickers (static)\n",
    "    tickers = []\n",
    "    if news.find('div', class_='taxonomy-links'):  \n",
    "        for ticker in news.find('div', class_='taxonomy-links').find_all('span', class_=re.compile(r'ticker-wrapper.*')):\n",
    "            \n",
    "            streamer = ticker.find('fin-streamer', {'data-field': 'regularMarketChangePercent'})\n",
    "            if streamer:\n",
    "                symbol = streamer.get('data-symbol')\n",
    "                change = streamer.get_text(strip=True)\n",
    "                \n",
    "                # Fallback to static if text is empty\n",
    "                if not change:\n",
    "                    change = None\n",
    "                tickers.append({'symbol': symbol, 'change': change})\n",
    "            else:\n",
    "                tickers.append({'symbol': ticker.text.strip(), 'change': None})     \n",
    "    news_data['tickers'] = tickers\n",
    "    \n",
    "    # Link\n",
    "    link = news.find('a')['href']\n",
    "    if not link: continue # If link is missing, skip this article\n",
    "    news_data['link'] = link\n",
    "    \n",
    "    # Fetch Article Details \n",
    "    try:\n",
    "        news_detail_url = news_data['link']\n",
    "        news_detail_page = requests.get(news_detail_url, headers=headers)\n",
    "        news_detail_soup = BeautifulSoup(news_detail_page.content, 'html.parser')\n",
    "        \n",
    "        # Author\n",
    "        author = news_detail_soup.find('div', class_=re.compile(r'byline-attr-author.*'))\n",
    "        news_data['authors'] = author.text.strip() if author else \"Unknown\"\n",
    "\n",
    "        # Time\n",
    "        time = news_detail_soup.find('time')\n",
    "        news_data['time_published'] = time.text if time else \"Unknown\"\n",
    "        \n",
    "        # Content\n",
    "        content_text = []\n",
    "        body_wrapper = news_detail_soup.find('div', class_='bodyItems-wrapper')\n",
    "        seen_texts = set()\n",
    "        \n",
    "        if body_wrapper:\n",
    "            for para in body_wrapper.find_all('p'):\n",
    "                text = para.text.strip()\n",
    "                if not text or text in seen_texts: continue\n",
    "                if \"Most Read from\" in text or \"Recommended Stories\" in text: break\n",
    "                if para.find_parent('div', class_='read-more-wrapper'): continue\n",
    "                \n",
    "                content_text.append(text)\n",
    "                seen_texts.add(text)\n",
    "                \n",
    "        news_data['content'] = \"\\n\".join(content_text)\n",
    "        \n",
    "        # Insert into MongoDB\n",
    "        if 'collection' in globals():\n",
    "            if collection.count_documents({'link': news_data['link']}) == 0:\n",
    "                collection.insert_one(news_data)\n",
    "                print(f\"Inserted: {news_data['title']}\")\n",
    "            else:\n",
    "                print(f\"Skipped: {news_data['title']}\")\n",
    "        else:\n",
    "            print(f\"Scraped (DB not connected): {news_data['title']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping details for {news_data['title']}: {e}\")\n",
    "    \n",
    "    print (\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
